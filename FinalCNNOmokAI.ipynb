{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI0w12A8wV0kofGBBIH+0B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y__qXXNkefV7",
        "outputId": "059b406c-3132-4679-b083-1968791344e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "_DoT72s9eodi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#move to path\n",
        "%cd /content/drive/MyDrive/Omok/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phguf1Bve1JU",
        "outputId": "d2aedb93-8c53-4eae-f270-e18e2b9ad1ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Omok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "jXgINFcDew8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 불러오고, 학습 데이터셋 생성\n",
        "\n",
        "w, h = 15, 15\n",
        "base_path = os.path.join('preprocessed_Data', '*.npz')\n",
        "\n",
        "file_list = glob(base_path)\n",
        "\n",
        "x_data, y_data = [], []\n",
        "for file_path in tqdm(file_list):\n",
        "    data = np.load(file_path)\n",
        "    x_data.extend(data['inputs'])\n",
        "    y_data.extend(data['outputs'])\n",
        "\n",
        "print(f\"x_data length: {len(x_data)}, y_data length: {len(y_data)}\")\n",
        "print(f\"First element shape in x_data: {x_data[0].shape}, First element shape in y_data: {y_data[0]}\")\n",
        "\n",
        "x_data = np.array(x_data, np.float32).reshape((-1, h, w, 1))\n",
        "y_data = np.array(y_data, np.float32).reshape((-1, h * w))\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.15, random_state=2024)\n",
        "\n",
        "del x_data, y_data\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "cBUPf5y6e0F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 구조 정의\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=(h, w, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(256, 7, activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(512, 3, activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, 7, activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Conv2D(1, 1, activation=None, padding='same'),\n",
        "    layers.Flatten(),  # Flatten 레이어 추가\n",
        "    layers.Dense(h * w, activation='sigmoid')  # 타겟 값의 형태에 맞추어 출력\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "'''\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=(h, w, 1)),\n",
        "    layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
        "    layers.Conv2D(256, 7, activation='relu', padding='same'),\n",
        "    layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
        "    layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
        "    layers.Conv2D(64, 7, activation='relu', padding='same'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Conv2D(1, 1, activation=None, padding='same'),\n",
        "    layers.Flatten(),  # Flatten 레이어 추가\n",
        "    layers.Dense(h * w, activation='sigmoid')  # 타겟 값의 형태에 맞추어 출력\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "'''"
      ],
      "metadata": {
        "id": "pd4Tb-mWfFQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "os.makedirs('models', exist_ok=True)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint('./models/%s.keras' % (start_time), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
        "        ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, mode='auto')\n",
        "    ],\n",
        "    validation_data=(x_val, y_val),\n",
        "    use_multiprocessing=True,\n",
        "    workers=16\n",
        ")"
      ],
      "metadata": {
        "id": "CpPOYRFJfNw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**추가 학습 (optional)**"
      ],
      "metadata": {
        "id": "tYowhv5dfWOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 기존에 학습된 모델 불러오기\n",
        "model = load_model('model.keras')\n",
        "\n",
        "print(len(model.layers))\n",
        "\n",
        "# 필요한 레이어만 재학습에 사용\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if i < 5 or i >= len(model.layers) - 8:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "model.layers[-2].trainable = True\n",
        "model.layers[-3].trainable = True\n",
        "model.layers[-4].trainable = True"
      ],
      "metadata": {
        "id": "OssikODGfP17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=256,\n",
        "    epochs=5,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint('./models/%s.keras' % (\"added_Model\"), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
        "        ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, mode='auto')\n",
        "    ],\n",
        "    validation_data=(x_val, y_val),\n",
        "    use_multiprocessing=True,\n",
        "    workers=16\n",
        ")"
      ],
      "metadata": {
        "id": "jPzqYJLFfeUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**keras 모델 파일을 json과 h5형식으로 변환**\n",
        "\n",
        "모델 구조와 가중치를 따로 저장해서 다른 플랫폼에서도 모델을 오류 없이 불러올 수 있도록 함."
      ],
      "metadata": {
        "id": "CpKvwfNXfho0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model, model_from_json\n",
        "\n",
        "# 기존 모델 로드 (에러 발생 가능성 있음)\n",
        "try:\n",
        "    model = load_model('./model.keras')\n",
        "except Exception as e:\n",
        "    print(\"Error loading model:\", e)\n",
        "    # 모델이 로드되지 않으면 종료\n",
        "    raise e\n",
        "\n",
        "# 모델 구조를 JSON 형식으로 저장\n",
        "model_json = model.to_json()\n",
        "with open(\"./model_structure.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 모델 가중치를 HDF5 형식으로 저장\n",
        "model.save_weights(\"./model_weight.h5\")\n",
        "print(\"Model saved in JSON and HDF5 formats.\")"
      ],
      "metadata": {
        "id": "A-oRCROAfyuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}